{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 598 Variational Autoencoders tutorial\n",
    "(This tutorial is based on this repository: https://github.com/jojonki/AutoEncoders/blob/master/vae.ipynb)\n",
    "\n",
    "<img src=\"vae.png\"/>\n",
    "\n",
    "The variational autoencoder (VAE) is a directed graphical model defined by a generative model $p_\\theta(\\mathbf{x}|\\mathbf{z})p_\\theta(\\mathbf{z})$ (solid lines), and an approximate posterior distribution $q_\\theta(\\mathbf{z}|\\mathbf{x})$(dotted lines) that approximates the true posterior $p_\\theta(\\mathbf{z}|\\mathbf{x})$. $q_\\theta(\\mathbf{z}|\\mathbf{x})$ is typically referred to as recognition model.\n",
    "The VAEs are trained by maximimizing the variational lower bound denoted as $\\mathcal{L}\\left(\\theta,\\phi;\\mathbf{x}^{\\left(i\\right)}\\right)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}\\right) &\\geq \\mathcal{L}\\left(\\theta,\\phi;\\mathbf{x}^{\\left(i\\right)}\\right) \\nonumber \\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}|\\mathbf{z}\\right)\\right] -D_{KL}\\left(q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)\\|p_{\\theta}\\left(\\mathbf{z}\\right)\\right), \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}|\\mathbf{z}\\right)\\right]$ measures the expected log-likelihood of the data point $\\mathbf{x}^{\\left(i\\right)}$ given the latent variable $\\mathbf{z}$ under the distribution $q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)$ defined by the recognition model. The second term,  $D_{KL}\\left(q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)\\|p_{\\theta}\\left(\\mathbf{z}\\right)\\right)$, measures how close the $q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)$ is to the prior distribution $p_{\\theta}\\left(\\mathbf{z}\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous definitions, let us build a VAE where the assumed distribution is Gaussian. Let us start by building the recognition model as a two-layer neural network that outputs the Gaussian parameters $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$. Note: To make things easier have the recognition network, output $\\text{log}\\ \\mathbf{\\sigma}^2$ (i.e., a simple linear output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "use_cuda = False\n",
    "input_size = 28 * 28\n",
    "units = 400\n",
    "batch_size = 32\n",
    "latent_size = 20 # z dim\n",
    "dummy_data = torch.from_numpy(1 / (1 + np.exp(-np.random.normal(size=(1, input_size))))).float()\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False,\n",
    "                       transform=transforms.ToTensor()),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to put variables in GPU or not\n",
    "# In our case we run this in CPU\n",
    "def to_var(x):\n",
    "    x = Variable(x)\n",
    "    if use_cuda:\n",
    "        x = x.cuda()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module): # 10 min\n",
    "    def __init__(self, input_size, latent_size, units):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.units = units\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # recognition model\n",
    "        self.fc1 = nn.Linear(input_size, self.units)\n",
    "        self.fc2 = nn.Linear(self.units, self.units)\n",
    "        self.layer_mu = nn.Linear(self.units, latent_size)\n",
    "        self.layer_logvar = nn.Linear(self.units, latent_size)\n",
    "\n",
    "    def recognition_model(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        z_mu = self.layer_mu(h2)\n",
    "        z_logvar = self.layer_logvar(h2)\n",
    "        return z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition model outputs shape passed successufully!\n"
     ]
    }
   ],
   "source": [
    "## CHECK THAT MU AND LOGVAR OUTPUT SHAPES MAKE SENSE\n",
    "model = VAE(input_size, latent_size, units)\n",
    "dummy_data = to_var(dummy_data)\n",
    "test_mu, test_logvar = model.recognition_model(dummy_data)\n",
    "assert list(test_mu.shape) == [1, latent_size] and list(test_logvar.shape) == [1, latent_size]\n",
    "print('Recognition model outputs shape passed successufully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us implement a function that, given the $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$ from the recognition model, we can use to sample a latent variable $\\mathbf{z}$. In VAEs, this sampling procedure is done by applying the Gaussian $\\textit{reparametrization}$ trick:\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\mathbf{z} = \\mathbf{\\mu} + \\mathbf{\\sigma} \\odot \\mathbf{\\epsilon}, \\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and $\\odot$ means elementwise multiplication. Note: The sample function takes in logvar so make sure you compute sigma when applying the reparametrization trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module): # 10 min\n",
    "    def __init__(self, input_size, latent_size, units):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.units = units\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # recognition model\n",
    "        self.fc1 = nn.Linear(input_size, self.units)\n",
    "        self.fc2 = nn.Linear(self.units, self.units)\n",
    "        self.layer_mu = nn.Linear(self.units, latent_size)\n",
    "        self.layer_logvar = nn.Linear(self.units, latent_size)\n",
    "\n",
    "    def recognition_model(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        z_mu = self.layer_mu(h2)\n",
    "        z_logvar = self.layer_logvar(h2)\n",
    "        return z_mu, z_logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        epsilon = Variable(mu.data.new(mu.size()).normal_())\n",
    "        sigma = torch.exp(0.5 * logvar)\n",
    "        z = mu + sigma * epsilon\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled z shape passed successufully!\n"
     ]
    }
   ],
   "source": [
    "## CHECK THAT Z OUTPUT SHAPE MAKES SENSE\n",
    "model = VAE(input_size, latent_size, units)\n",
    "test_z = model.reparametrize(test_mu, test_logvar)\n",
    "assert list(test_z.shape) == [1, latent_size]\n",
    "print('Sampled z shape passed successufully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us implement a two-layer generative model $p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}|\\mathbf{z}\\right)$. For this exercise, we are assuming a bernoulli distribution in the output. Therefore, make sure your output is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module): # 10 min\n",
    "    def __init__(self, input_size, latent_size, units):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.units = units\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # recognition model\n",
    "        self.fc1 = nn.Linear(input_size, self.units)\n",
    "        self.fc2 = nn.Linear(self.units, self.units)\n",
    "        self.layer_mu = nn.Linear(self.units, latent_size)\n",
    "        self.layer_logvar = nn.Linear(self.units, latent_size)\n",
    "\n",
    "        # generation model\n",
    "        self.fc3 = nn.Linear(latent_size, self.units)\n",
    "        self.fc4 = nn.Linear(self.units, self.units)\n",
    "        self.layer_output = nn.Linear(self.units, input_size)\n",
    "          \n",
    "    def recognition_model(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        z_mu = self.layer_mu(h2)\n",
    "        z_logvar = self.layer_logvar(h2)\n",
    "        return z_mu, z_logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        epsilon = Variable(mu.data.new(mu.size()).normal_())\n",
    "        sigma = torch.exp(0.5 * logvar)\n",
    "        z = mu + sigma * epsilon\n",
    "        return z\n",
    "\n",
    "    def generation_model(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        h4 = self.relu(self.fc4(h3))\n",
    "        x_hat = self.sigmoid(self.layer_output(h4))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation model outputs shape passed successufully!\n"
     ]
    }
   ],
   "source": [
    "## CHECK THAT GENERATION MODEL OUTPUT SHAPES MAKE SENSE\n",
    "model = VAE(input_size, latent_size, units)\n",
    "test_output = model.generation_model(test_z)\n",
    "assert list(test_output.shape) == [1, input_size]\n",
    "print('Generation model outputs shape passed successufully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us put everything together as a feedforward neural network. Specifically, first infer the latent variable z using the recognition_model function, and then follow by generating $\\mathbf{x}^{(i)}$ using the generation_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, units):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.units = units\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # recognition model\n",
    "        self.fc1 = nn.Linear(input_size, self.units)\n",
    "        self.fc2 = nn.Linear(self.units, self.units)\n",
    "        self.layer_mu = nn.Linear(self.units, latent_size)\n",
    "        self.layer_logvar = nn.Linear(self.units, latent_size)\n",
    "\n",
    "        # generation model\n",
    "        self.fc3 = nn.Linear(latent_size, self.units)\n",
    "        self.fc4 = nn.Linear(self.units, self.units)\n",
    "        self.layer_output = nn.Linear(self.units, input_size)\n",
    "            \n",
    "    def recognition_model(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        z_mu = self.layer_mu(h2)\n",
    "        z_logvar = self.layer_logvar(h2)\n",
    "        return z_mu, z_logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        epsilon = Variable(mu.data.new(mu.size()).normal_())\n",
    "        sigma = logvar.mul(0.5).exp_()\n",
    "        z = mu + sigma * epsilon\n",
    "        return z\n",
    "\n",
    "    def generation_model(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        h4 = self.relu(self.fc4(h3))\n",
    "        x_hat = self.sigmoid(self.layer_output(h4))\n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.recognition_model(x.view(-1, input_size))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        x_hat = self.generation_model(z)\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full VAE model outputs shape passed successufully!\n"
     ]
    }
   ],
   "source": [
    "## CHECK THAT VAE MODEL OUTPUT SHAPES MAKE SENSE\n",
    "model = VAE(input_size, latent_size, units)\n",
    "test_output, test_mu, test_logvar = model.forward(dummy_data)\n",
    "assert list(test_output.shape) == [1, input_size] and list(test_mu.shape) == [1, latent_size] and list(test_logvar.shape) == [1, latent_size]\n",
    "print('Full VAE model outputs shape passed successufully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to implement the objective function. Specifically, we want to maximize $\\mathcal{L}\\left(\\theta,\\phi;\\mathbf{x}^{\\left(i\\right)}\\right)$. To implement it as a minimization problem we want to minimize $-\\mathcal{L}\\left(\\theta,\\phi;\\mathbf{x}^{\\left(i\\right)}\\right)$. Therefore, we want to implement $-\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}|\\mathbf{z}\\right)\\right]$ which, $\\textbf{in this tutorial}$, turns out to be the binary cross-entropy between a target output and the output generated (Note: when we work with real values we use a Gaussian distribution which turns out to be MSE):\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}^{\\left(i\\right)}|\\mathbf{z}\\right)\\right] = BCE(\\mathbf{x}^{(i)}, \\hat{\\mathbf{x}}^{(i)}) \\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "Now, as defined before, the second term in the variational lower bound is $D_{KL}\\left(q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)\\|p_{\\theta}\\left(\\mathbf{z}\\right)\\right)$. In traditional VAE, $p_{\\theta}\\left(\\mathbf{z}\\right)$ is assumed to be a standard gaussian distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. Therefore, the objective turns out to be:\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_{KL}\\left(q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}^{\\left(i\\right)}\\right)\\|p_{\\theta}\\left(\\mathbf{z}\\right)\\right) = -\\frac{1}{2}\\sum_{j=1}^{J}\\left(1+\\log\\left(\\sigma_{j}^{2}\\right)-\\mu_{j}^{2}-\\sigma_{j}^{2}\\right) \\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mu_j$ and $\\sigma_j$ are the $j$-th element in the recognition network outputs. Given this, let us implement the objective function (None: remember the output of the recognition network is $\\text{log} \\sigma^2$ so you need to convert it to $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar): # 5 min\n",
    "    BCE = F.binary_cross_entropy(x_hat, x.view(-1, input_size), size_average=False)\n",
    "    DKL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (BCE + DKL) / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss output shape passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "## CHECK THAT LOSS OUTPUT SHAPE IS OF A SINGLE VALUE\n",
    "test_loss = loss_function(test_output, dummy_data, test_mu, test_logvar)\n",
    "assert len(test_loss.shape) == 0\n",
    "print('Loss output shape passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = to_var(data)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data.numpy()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data.numpy() / len(data)))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, labels) in enumerate(test_loader):\n",
    "        data = to_var(data)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                     'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full VAE model\n",
    "model = VAE(input_size, latent_size, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 17.054039\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 4.867999\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4.162251\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 4.011993\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.799066\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 4.142150\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 3.526347\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 3.280792\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3.151355\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 3.447148\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 3.437461\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 3.387831\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 3.496448\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 3.477020\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 3.355832\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 3.521839\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3.446627\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 2.944883\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 3.444725\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 3.633421\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3.417958\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 3.224861\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 3.326081\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 3.416745\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 3.318017\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 3.409523\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 3.384019\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 3.310503\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 3.519373\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 3.193774\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.953700\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 3.202029\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.918973\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 3.023730\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 3.491535\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 3.048763\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sample some data\n",
    "model.eval()\n",
    "\n",
    "# Get some random z\n",
    "z_random = torch.from_numpy(np.random.normal(size=(10, latent_size))).float()\n",
    "samples = model.generation_model(z_random).data.cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(10, 10)\n",
    "gs.update(wspace=0.05, hspace=0.05)\n",
    "for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a transition from labels[0] to labels[1]\n",
    "model.eval()\n",
    "for i, (data, labels) in enumerate(test_loader):\n",
    "    data = to_var(data)\n",
    "    mu, logvar = model.recognition_model(data.view(-1, input_size))\n",
    "    z = model.reparametrize(mu, logvar)\n",
    "    print('Transition labels:', labels[0], '-->', labels[1])\n",
    "    z_cont = to_var(torch.zeros(10, latent_size))\n",
    "    for i in range(10):\n",
    "        t = 1.0 - i/9\n",
    "        z_cont[i] = t * z[0] + (1-t) * z[1]\n",
    "    samples = model.generation_model(z_cont).data.cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = gridspec.GridSpec(10, 10)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model.eval()\n",
    "z_list = None\n",
    "l_list = []\n",
    "for i, (data, labels) in enumerate(test_loader):\n",
    "    data = to_var(data)\n",
    "    mu, logvar = model.recognition_model(data.view(-1, 28*28))\n",
    "    z = mu\n",
    "    if i == 0:\n",
    "        z_list = z\n",
    "        l_list = labels\n",
    "    else:\n",
    "        z_list = torch.cat((z_list, z), 0)\n",
    "        l_list = torch.cat((l_list, labels), 0)\n",
    "\n",
    "z_list = z_list.data.cpu().numpy()[:1000]\n",
    "l_list = l_list.cpu().numpy()[:1000] # labels are not Variable\n",
    "\n",
    "# Visualization using TSNE\n",
    "X_reduced = TSNE(n_components=2, random_state=0).fit_transform(z_list)\n",
    "\n",
    "print (X_reduced.shape)\n",
    "# (N, 2)\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'pink', 'orange', 'purple'\n",
    "for i, c in enumerate(colors):\n",
    "    plt.scatter(X_reduced[l_list == i, 0], X_reduced[l_list == i, 1], c=c, label=str(i))\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=l_list)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You just built a variational autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
